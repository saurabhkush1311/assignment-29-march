{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Lasso Regression, also known as L1 regularization, is a linear regression technique that incorporates a penalty term equivalent to the absolute value of the coefficients. It adds a regularization term to the least squares loss function, which helps in feature selection and prevents overfitting. Lasso Regression differs from other regression techniques, such as ordinary least squares regression or Ridge Regression, by introducing a sparsity-inducing regularization term. This regularization encourages the model to set some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "Q2. The main advantage of using Lasso Regression in feature selection is its ability to shrink the coefficients of irrelevant or less important features to zero. By setting certain coefficients to zero, Lasso Regression performs automatic feature selection, which helps in reducing the complexity of the model and focusing on the most important features. This feature selection property makes Lasso Regression particularly useful when dealing with high-dimensional datasets containing a large number of features.\n",
    "\n",
    "Q3. The coefficients of a Lasso Regression model can be interpreted in the following way: \n",
    "- Non-zero coefficients: If a coefficient is non-zero, it means that the corresponding feature has a significant effect on the target variable. The sign and magnitude of the coefficient indicate the direction and strength of the relationship between the feature and the target variable. \n",
    "- Zero coefficients: If a coefficient is exactly zero, it means that the corresponding feature has been excluded from the model. This suggests that the feature is not relevant in predicting the target variable, and it can be considered as effectively removed from the model.\n",
    "\n",
    "Q4. In Lasso Regression, the main tuning parameter is the regularization parameter, often denoted as λ (lambda). This parameter controls the strength of the regularization applied to the model. By adjusting the value of λ, you can control the balance between fitting the data well (reducing the residual sum of squares) and minimizing the absolute values of the coefficients (promoting sparsity). A larger λ value increases the amount of shrinkage applied to the coefficients, resulting in more coefficients being pushed towards zero. Conversely, a smaller λ value reduces the amount of shrinkage, allowing more coefficients to remain non-zero.\n",
    "\n",
    "Q5. Lasso Regression is primarily a linear regression technique and is suitable for linear regression problems. However, Lasso Regression can be used for non-linear regression problems by incorporating non-linear transformations of the input features. By introducing non-linear transformations, such as polynomial terms or interaction terms, the model can capture non-linear relationships between the features and the target variable. The transformed features can then be used as inputs to the Lasso Regression model.\n",
    "\n",
    "Q6. The main difference between Ridge Regression and Lasso Regression lies in the type of regularization they employ. Ridge Regression uses L2 regularization, which adds a penalty term equivalent to the square of the coefficients to the least squares loss function. This leads to all coefficients being shrunk towards zero, but they are rarely exactly zero. In contrast, Lasso Regression uses L1 regularization, which adds a penalty term equivalent to the absolute value of the coefficients. This allows some coefficients to be exactly zero, resulting in automatic feature selection. Therefore, Lasso Regression can perform both regularization and feature selection simultaneously, whereas Ridge Regression primarily focuses on regularization.\n",
    "\n",
    "Q7. Yes, Lasso Regression can handle multicollinearity in the input features. Multicollinearity occurs when there is a high correlation between two or more predictor variables in a regression model. Lasso Regression addresses multicollinearity by effectively selecting one of the correlated features and setting the coefficients of the remaining correlated features to zero. By doing so, Lasso Regression automatically chooses one representative feature from the correlated group, reducing the impact of multicollinearity on the model's performance.\n",
    "\n",
    "Q8. The optimal value of the regularization parameter (λ) in Lasso Regression can be determined using techniques like cross-validation. The process typically involves selecting a range of λ values and evaluating the performance of the model for each value using a validation set or through cross-validation. The optimal value of λ is often chosen based on criteria such as minimizing the mean squared error (MSE) or optimizing a scoring metric like the Akaike information criterion (AIC) or Bayesian information criterion (BIC). Cross-validation helps in assessing the model's performance across different values of λ and choosing the one that provides the best balance between fitting the data and promoting sparsity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
